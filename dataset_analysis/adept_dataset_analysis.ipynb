{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Adept**\n",
        "\n",
        "This is an overview of the Adept dataset first introduced by Emami et al. in [ADEPT: An Adjective-Dependent Plausibility Task](https://aclanthology.org/2021.acl-long.553/) (2021).\n",
        "\n",
        "The overview investigates the statistical distributions of the dataset features such as labels and sentence length distributions in order to provide an introductory but informative look at the data.\n",
        "\n",
        "**By team Tennant: Anna Golub, Beate Zywietz**"
      ],
      "metadata": {
        "id": "iCHjb5mqu5av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "velMM6EN5qPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lTVjC1_Tg6b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the data"
      ],
      "metadata": {
        "id": "f3LyBPcS7KN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into train set, dev (validation) set and test set."
      ],
      "metadata": {
        "id": "IJKhG4cg7eNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_json('../adept/train-dev-test-split/train.json')\n",
        "dev = pd.read_json('../adept/train-dev-test-split/val.json')\n",
        "test = pd.read_json('../adept/train-dev-test-split/test.json')"
      ],
      "metadata": {
        "id": "0w8fKgPawGLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset sizes"
      ],
      "metadata": {
        "id": "8rR5Atou7PZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train:', train.shape[0])\n",
        "print('Dev:', dev.shape[0])\n",
        "print('Test:', test.shape[0])"
      ],
      "metadata": {
        "id": "6_qF16PiwTFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset features:\n",
        "* sentence1 - plain sentence\n",
        "* sentence2 - sentence with modifier\n",
        "* modifier\n",
        "* noun that is being modified\n",
        "* class label\n",
        "* idx - data point index"
      ],
      "metadata": {
        "id": "mGUqINtt7rNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "-5XesSDg7qNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing values - none found"
      ],
      "metadata": {
        "id": "IRadP9GbFUv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isna().sum().sum(), dev.isna().sum().sum(), test.isna().sum().sum()"
      ],
      "metadata": {
        "id": "3V8ZPGfMFXLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label'] == 4].head()"
      ],
      "metadata": {
        "id": "GLZBpX75bv8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label distribution\n",
        "0 - very implausible  \n",
        "4 - very plausible\n",
        "\n",
        "As shown by the bar chart below, most of the data lies in the middle of the scale (the annotators were unsure about how plausible those sentences are). The rest of the data is significantly skewed towards the non-plausible end. The label 4 (very plausible) is only represented by 65 examples."
      ],
      "metadata": {
        "id": "hN7C1T7sxSS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = train['label'].value_counts().reset_index().rename(\n",
        "    columns={'index': 'label', 'label': 'count'}\n",
        ").sort_values(by='label')\n",
        "label_counts"
      ],
      "metadata": {
        "id": "5zfqM3KN-zql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_total = train['label'].value_counts().sum()\n",
        "label_counts['bar_chart_labels'] = label_counts['count'].apply(\n",
        "    lambda x: '< 1%' if x / labels_total < 0.01 else '{:2.2%}'.format(x / labels_total)\n",
        ")"
      ],
      "metadata": {
        "id": "y2vqFZiYAYVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.barplot(data=label_counts, x='label', y='count', color='b');\n",
        "ax.bar_label(ax.containers[0], labels=label_counts['bar_chart_labels']);"
      ],
      "metadata": {
        "id": "10067i4Hwai3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouns & modifiers"
      ],
      "metadata": {
        "id": "b2jCAAbqCUXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noun distribution"
      ],
      "metadata": {
        "id": "uuJ_XckDfrHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at noun occurrences"
      ],
      "metadata": {
        "id": "Cnb2cOohEPOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_uniq_noun = list(train['noun'].unique())\n",
        "dev_uniq_noun = list(dev['noun'].unique())\n",
        "test_uniq_noun = list(test['noun'].unique())\n",
        "\n",
        "print(f'Overall: {len(set(train_uniq_noun + dev_uniq_noun + test_uniq_noun))} unique nouns')\n",
        "print(f'Train set: {len(train_uniq_noun)} unique nouns')\n",
        "print(f'Dev set: {len(dev_uniq_noun)} unique nouns;  {len([m for m in dev_uniq_noun if m not in train_uniq_noun])} of them are NOT in train')\n",
        "print(f'Test set: {len(test_uniq_noun)} unique nouns;  {len([m for m in test_uniq_noun if m not in train_uniq_noun])} of them are NOT in train')"
      ],
      "metadata": {
        "id": "uqsoaLOSEPOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which nouns are the most common in the training set,\n",
        "and how common are they in the development and test set?"
      ],
      "metadata": {
        "id": "yQ4eVFHREREn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Most common nouns\\n')\n",
        "n = 5  # number of instances to show\n",
        "print(f\"train:\\n{train['noun'].value_counts().nlargest(n)}\")\n",
        "print(f\"\\ntest:\\n{test['noun'].value_counts().nlargest(n)}\")\n",
        "\n",
        "# count how often the most common nouns from the training set appear in the other sets,\n",
        "# relative to the size of the split\n",
        "print(\"\\nnoun\\ttrain\\tdev\\ttest\")\n",
        "for noun, train_val in train['noun'].value_counts()[:n].to_dict().items():\n",
        "    if noun in dev['noun'].to_dict().values():\n",
        "        dev_val = dev['noun'].value_counts()[noun]\n",
        "    else: dev_val = 0\n",
        "    if noun in test['noun'].to_dict().values():\n",
        "        test_val = test['noun'].value_counts()[noun]\n",
        "    else: test_val = 0\n",
        "    train_val = train_val/train['noun'].size\n",
        "    dev_val = dev_val/dev['noun'].size\n",
        "    test_val = test_val/test['noun'].size\n",
        "    print('{}\\t{:2.2%}\\t{:2.2%}\\t{:2.2%}'.format(noun, train_val, dev_val, test_val))"
      ],
      "metadata": {
        "id": "IF8e6-vSftTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the exeption of the word 'menu' the most frequent nouns are different between the three sets. No single noun seems overly common in any set, with the most frequent noun in the training set only appearing in less than 1% of all instances."
      ],
      "metadata": {
        "id": "sF4UsxGim5-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifier distribution"
      ],
      "metadata": {
        "id": "e-jL3LbIfzBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at modifier occurrences"
      ],
      "metadata": {
        "id": "aJFtneEjxrHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_uniq_mod = list(train['modifier'].unique())\n",
        "dev_uniq_mod = list(dev['modifier'].unique())\n",
        "test_uniq_mod = list(test['modifier'].unique())\n",
        "\n",
        "print(f'Overall: {len(set(train_uniq_mod + dev_uniq_mod + test_uniq_mod))} unique modifiers')\n",
        "print(f'Train set: {len(train_uniq_mod)} unique modifiers')\n",
        "print(f'Dev set: {len(dev_uniq_mod)} unique modifiers;  {len([m for m in dev_uniq_mod if m not in train_uniq_mod])} of them are NOT in train')\n",
        "print(f'Test set: {len(test_uniq_mod)} unique modifiers;  {len([m for m in test_uniq_mod if m not in train_uniq_mod])} of them are NOT in train')"
      ],
      "metadata": {
        "id": "322kFN-NxfcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which modifiers are the most common in the training set,\n",
        "and how common are they in the development and test set?"
      ],
      "metadata": {
        "id": "9Q2NbJSTCrNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Most common modifiers\\n')\n",
        "\n",
        "n = 5  # number of instances to show\n",
        "print(f\"train:\\n{train['modifier'].value_counts().nlargest(n)}\\n\")\n",
        "print(f\"test:\\n{test['modifier'].value_counts().nlargest(n)}\")\n",
        "\n",
        "# count how often the most common modifiers from the training set appear in the other sets,\n",
        "# relative to the size of the split\n",
        "print(\"\\nmod\\ttrain\\tdev\\ttest\")\n",
        "for mod, train_val in train['modifier'].value_counts()[:n].to_dict().items():\n",
        "    if mod in dev['modifier'].to_dict().values():\n",
        "        dev_val = dev['modifier'].value_counts()[mod]\n",
        "    else: dev_val = 0\n",
        "    if mod in test['modifier'].to_dict().values():\n",
        "        test_val = test['modifier'].value_counts()[mod]\n",
        "    else: test_val = 0\n",
        "    train_val = train_val / train['modifier'].size\n",
        "    dev_val = dev_val / dev['modifier'].size\n",
        "    test_val = test_val / test['modifier'].size\n",
        "    print('{}\\t{:2.2%}\\t{:2.2%}\\t{:2.2%}'.format(mod, train_val, dev_val, test_val))"
      ],
      "metadata": {
        "id": "1UU0XILHf08F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though there are more different modifiers than nouns, the common modifiers appear more frequently relative to the size of the dataset (~3% instead of <1%). The modifiers that are frequent in one set are also frequent in the other sets."
      ],
      "metadata": {
        "id": "5A7ItDtRnUUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noun-modifier distribution\n",
        "Most common noun-modifier combinations"
      ],
      "metadata": {
        "id": "6q5SdXiJf6Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noun_mod = {}  # keys are (noun, mod) tuples, values are the number of their appearances\n",
        "for i in range(train['noun'].size):\n",
        "    t = (train['noun'][i], train['modifier'][i])\n",
        "    if noun_mod.get(t):\n",
        "        noun_mod[t] += 1\n",
        "    else:\n",
        "        noun_mod[t] = 1\n",
        "nm_series = pd.Series(noun_mod)\n",
        "print(nm_series.sort_values(ascending=False)[:5])"
      ],
      "metadata": {
        "id": "mxpMtNapf7Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentences"
      ],
      "metadata": {
        "id": "5d7HeH1dgAXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unique sentences"
      ],
      "metadata": {
        "id": "qnRqEOJfFtMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the number of plain sentences (sentence1): almost all sentences only occur once. However, not all. There's some overlap between the train and dev, test sets."
      ],
      "metadata": {
        "id": "7nTF-1Rhxzk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_uniq_sent = list(train['sentence1'].unique())\n",
        "dev_uniq_sent = list(dev['sentence1'].unique())\n",
        "test_uniq_sent = list(test['sentence1'].unique())\n",
        "\n",
        "print(f'Overall: {len(set(train_uniq_sent + dev_uniq_sent + test_uniq_sent))} unique sentences')\n",
        "print(f'Train set: {len(train_uniq_sent)} unique sentences')\n",
        "print(f'Dev set: {len(dev_uniq_sent)} unique sentences;  {len([m for m in dev_uniq_sent if m in train_uniq_sent])} of them ARE in train')\n",
        "print(f'Test set: {len(test_uniq_sent)} unique sentences;  {len([m for m in test_uniq_sent if m in train_uniq_sent])} of them ARE in train')"
      ],
      "metadata": {
        "id": "r8_IALq6xmdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that all modified sentences (sentence2) are unique - no!\n",
        "\n",
        "There are duplicates within the train set, that is some sentences are recorded multiple times with the same OR different labels.\n",
        "\n",
        "There is a 4-sentence overlap between train and dev and 1-sentence overlap between train and test. These can be used for sanity checks later on during model training."
      ],
      "metadata": {
        "id": "VkBekvxRGbQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['set'] = 'train'\n",
        "dev['set'] = 'dev'\n",
        "test['set'] = 'test'\n",
        "df = pd.concat([train, dev, test])\n",
        "sent_counts = df['sentence2'].value_counts().sort_values(ascending=False).reset_index().rename(\n",
        "    columns={'index': 'sentence2', 'sentence2': 'count'}\n",
        ")\n",
        "df = df.merge(sent_counts, on='sentence2')\n",
        "df[df['count'] > 1][['sentence2', 'set', 'label']]"
      ],
      "metadata": {
        "id": "Yfbg3HblZo_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence length"
      ],
      "metadata": {
        "id": "uCjyJTDUFwWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since some ML models struggle with long sentences, we decided to find the longest sentences in the dataset. Their length is calculated based on their character count, including spaces."
      ],
      "metadata": {
        "id": "0az00NRwElci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_len = {}  # keys are the index (not the idx) of each sentence, values are their character count\n",
        "for i in range(train['sentence2'].size):\n",
        "    sentence_len[i] = len(train['sentence2'][i])\n",
        "sl_series = pd.Series(sentence_len)\n",
        "for k, v in sl_series.nlargest(5).items():\n",
        "    print(v, train['sentence2'][k])"
      ],
      "metadata": {
        "id": "GIy8ddAYgBRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean sentence length"
      ],
      "metadata": {
        "id": "VJU7yyJBH0I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sl_series.mean()"
      ],
      "metadata": {
        "id": "azJ4XYvDH1ja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}